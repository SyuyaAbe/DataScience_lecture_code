{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章データへの前処理\n",
    "今回のサンプルでは、下記のように文書への前処理を行います。<br>\n",
    "\n",
    "- 後の扱いを考え、文字コードをUTF-8に統一（`Webスクレイピング時点で実施済み`）\n",
    "- Pythonライブラリ BeautifulSoup を使用しhtmlタグを除去（`Webスクレイピング時点で実施済み`）\n",
    "- 表記のゆらぎの修正　→　Pythonライブラリ neologdn を使用\n",
    "- 形態素解析ソフト Janome を使用し、単語分割し不要単語を除去する\n",
    "\n",
    "今回のターゲットは Naive Bayes での文書分類なので、名詞などの判定につながりそうな単語だけに絞ることがポイント。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neologdn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9216fbed0cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m## 以下を使用するには事前準備が必要\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneologdn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjanome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neologdn'"
     ]
    }
   ],
   "source": [
    "## 必要なライブラリの読み込み\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "## 以下を使用するには事前準備が必要\n",
    "import neologdn\n",
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 日本語文書の形態素解析の動作確認\n",
    "janome の Tokenizer がどのような動作をするか確認します。\n",
    "- node.surface → 分割した単語そのもの\n",
    "- node.part_of_speech → node.surfaceに対応するcsv文字列: 品詞,品詞細分類1,分類2,分類3\n",
    "\n",
    "品詞や細分類にどのような項目が入ってくるかは、「IPA品詞体系」を検索し参照してみて下さい。 →参考： [http://www.unixuser.org/~euske/doc/postag/](http://www.unixuser.org/~euske/doc/postag/)<br>\n",
    "（**janome は分析結果をIPA品詞体系で返します**）<br><br>\n",
    "neologdn の動作も確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist = list(Tokenizer().tokenize('隣の客はよく柿食う客だ。'))\n",
    "for node in nodelist:\n",
    "    print('--------')\n",
    "    for key, value in node.__dict__.items():\n",
    "        print(key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neologdn.normalize('わーい。わーーーい。')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess_text_1\n",
    "与えられた文書の文字列（input_text）を前処理・単語分割し、必要な単語の羅列（List型）を返す関数を定義する。<br>\n",
    "※ この処理はあくまで１例<br>\n",
    "### preprocess_file_1\n",
    "与えられたファイル（複数）の中身の文書テキストを preprocess_text_1 で前処理し、結果の単語の羅列（List型）をまとめてリストとして返す関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_1(input_text):\n",
    "    input_text_reg = neologdn.normalize(input_text)  ### neologdn による表記ゆれ正規化 -> input_text_reg\n",
    "    nodelist = Tokenizer().tokenize(input_text_reg)  ### input_text_reg を形態素解析 -> nodelist\n",
    "    res = []                                         ### 結果の戻り値用入れ物\n",
    "    ### 形態素解析した結果（単語）でループして処理する\n",
    "    for node in nodelist:\n",
    "        ### node.surface は 分割した単語そのもの　　例：「隣」\n",
    "        sur = node.surface\n",
    "        ### node.part_of_speech はsurfaceに対応するcsv文字列: 品詞,品詞細分類1,分類2,分類3\n",
    "        a = node.part_of_speech.split(',')\n",
    "        hin = a[0]           ### 品詞　　　　　　例：「名詞」\n",
    "        bun1 = a[1]          ### 品詞細分類1　　 例：「一般」\n",
    "        if hin == '動詞':    ### 動詞は原形(node.base_form)に直しておく\n",
    "            sur = node.base_form\n",
    "        ### まずは単語全部を対象とする\n",
    "        taisho = True        \n",
    "        ### 品詞が記号や助詞だったら対象から外す\n",
    "        if hin in ['BOS/EOS','連体詞','接続詞','助詞','助動詞','連語','副詞','接頭詞','記号']:\n",
    "            taisho = False\n",
    "        ### 品詞細分類1が下記のものだったら対象から外す\n",
    "        if bun1 in ['形容動詞語幹','副詞可能','代名詞','ナイ形容詞語幹','特殊','数','接尾','非自立']:\n",
    "            taisho = False\n",
    "        ### 名詞のうち、数字入りだったり、カッコが入ってしまっているものは対象から外す\n",
    "        if hin == '名詞' and ( re.search('\\d',sur) or re.search('[\\(\\)\\（）]',sur) ):\n",
    "            taisho = False\n",
    "        ### 長さが1文字のものは判定用の単語とみなさない。対象から外す\n",
    "        if len(sur) <= 1:\n",
    "            taisho = False\n",
    "        ### 特定の単語が出てきたら、対象から外す\n",
    "        #if sur in ['判定には不適切な単語１','判定には不適切な単語２']:\n",
    "        #    taisho = False\n",
    "        \n",
    "        # taisho = True  ### これは全部を対象にしてみる テスト用\n",
    "        ### それでも対象として残ったものを出力する\n",
    "        if taisho == True:\n",
    "            res.append(sur)\n",
    "    return res\n",
    "\n",
    "def preprocess_file_1(input_files):\n",
    "    res = []\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, 'r', encoding='utf-8') as input_file_handle:\n",
    "            res += preprocess_text_1( input_file_handle.read() )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webスクレイピングで取得したファイルを前処理して、１行１単語の前処理済みテキストとして書き出す。<br>\n",
    "- mlit_y28.txt , mlit_y29.txt の２つを前処理 → mlit.txt へ書き込む\n",
    "- env_y28.txt , env_y29.txt の２つを前処理 → env.txt へ書き込む <br>\n",
    "ここで作った mlit.txt と env.txt は学習の為のデータ（単語を数える）として使います。<br>\n",
    "glob.globを使うと、存在するファイルのリストを作るのに便利です。又、List型を文字列として連結する際はjoinを使うのが便利です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_m = preprocess_file_1(glob.glob('mlit_y2[89].txt'))\n",
    "with open('mlit.txt', 'w', encoding='utf-8') as mlit_out_file:\n",
    "    mlit_out_file.write( \"\\n\".join( words_m ) + \"\\n\" )\n",
    "\n",
    "words_e = preprocess_file_1(glob.glob('env_y2[89].txt'))\n",
    "with open('env.txt', 'w', encoding='utf-8') as env_out_file:\n",
    "    env_out_file.write( \"\\n\".join( words_e ) + \"\\n\")\n",
    "\n",
    "[glob.glob('mlit.txt'),glob.glob('env.txt')]    # ファイルが出来たか確認用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証用データ（判定する文書）として使う為、mlit,envの両方で、<br>\n",
    "30年度のテキストの中ランダムな場所から、全体の1%を抜き出しておきます。<br>\n",
    "配列(List)の範囲指定を使って、切り出しをします。 （例） hairetsu[50:100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.01      # = 1%\n",
    "\n",
    "with open('mlit_check.txt', 'w', encoding='utf-8') as mlit_out_file:\n",
    "    a = preprocess_file_1(['mlit_y30.txt'])\n",
    "    r = random.random()\n",
    "    mlit_out_file.write( (\"\\n\".join(a[int(r * (1.0 - test_ratio)):][0:int(test_ratio * len(a))])) + \"\\n\" )\n",
    "\n",
    "with open('env_check.txt', 'w', encoding='utf-8') as env_out_file:\n",
    "    a = preprocess_file_1(['env_y30.txt'])\n",
    "    r = random.random()\n",
    "    env_out_file.write( (\"\\n\".join(a[int(r * (1.0 - test_ratio)):][0:int(test_ratio * len(a))])) + \"\\n\" )\n",
    "\n",
    "[glob.glob('mlit_check.txt'),glob.glob('env_check.txt')]    # ファイルが出来たか確認用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### うまくいかない場合\n",
    "***ここはうまくいっていれば実行不要です***\n",
    "\n",
    "ここまでうまくいっていれば、<br>\n",
    "mlit.txt , mlit_check.txt , env.txt , env_check.txt の4つが作成されているはずです。<br>\n",
    "開いて確認してみてください。<br>\n",
    "必要なライブラリのインストールができなかった場合は、上記4つのファイルを<br>\n",
    "既に準備してあるものの中から下記を実行してコピーします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil,glob\n",
    "target = ['mlit.txt','mlit_check.txt','env.txt','env_check.txt']\n",
    "for f in target:\n",
    "    shutil.copy('file/' + f,'./')\n",
    "    print(glob.glob(f))  # ファイルが出来たことを確認"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
