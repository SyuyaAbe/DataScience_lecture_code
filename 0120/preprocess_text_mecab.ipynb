{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章データへの前処理\n",
    "今回のサンプルでは、下記のように文書への前処理を行います。<br>\n",
    "\n",
    "- 後の扱いを考え、文字コードをUTF-8に統一（`Webスクレイピング時点で実施済み`）\n",
    "- Pythonライブラリ BeautifulSoup を使用しhtmlタグを除去（`Webスクレイピング時点で実施済み`）\n",
    "- 表記のゆらぎの修正　→　Pythonライブラリ neologdn を使用\n",
    "- 形態素解析ソフト MeCab を使用し、単語分割し不要ワードを除去する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 必要なライブラリの読み込み\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import glob\n",
    "## 【注】以下の2つを使用するには事前準備が必要（別途資料参照）\n",
    "import MeCab\n",
    "import neologdn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess_text_1\n",
    "与えられた文書の文字列（input_text）を前処理・単語分割し、必要な単語の羅列（List型）を返す関数を定義する。<br>\n",
    "※ この処理はあくまで１例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_1(input_text):\n",
    "    input_text_reg = neologdn.normalize(input_text)     ### neologdn による表記ゆれ正規化 -> input_text_reg\n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")                   ### MeCab による日本語形態素解析\n",
    "    node = tagger.parseToNode(input_text_reg)           ###    -> input_text_reg を解析した結果がnodeに入る\n",
    "    res = []                                            ### 結果の戻り値用入れ物\n",
    "    while node is not None:\n",
    "        ### nodeの中にはfeatureプロパティ、surfaceプロパティがある\n",
    "        sur = node.surface   ### node.surface は 分割した単語そのもの\n",
    "        ### node.feature はsurfaceに対応するcsv文字列: 品詞,品詞細分類1,分類2,分類3,活用形,活用型,原形,読み,発音\n",
    "        a = node.feature.split(',')\n",
    "        hin = a[0]           ### 品詞\n",
    "        bun1 = a[1]          ### 品詞細分類1\n",
    "        if hin == '動詞':    ### 動詞は原形に直しておく\n",
    "            sur = a[6]\n",
    "        ### 品詞、細分類1 で不要そうなカテゴリのもの、数字や記号カッコ入り、長さが１、の場合を除外し、それ以外の単語を拾う\n",
    "        if not( ( hin in ['BOS/EOS','連体詞','接続詞','助詞','助動詞','連語','副詞','接頭詞','記号'] ) or \\\n",
    "           ( bun1 in ['形容動詞語幹','副詞可能','代名詞','ナイ形容詞語幹','特殊','数','接尾','非自立'] ) or \\\n",
    "           ( hin == '名詞' and ( re.search('\\d',sur) or re.search('[\\(\\)\\（）]',sur) ) ) or len(sur) <= 1 ) :\n",
    "            res.append(sur)\n",
    "        node = node.next\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess_file_1\n",
    "与えられたファイル（複数）の中身の文書テキストを preprocess_text_1 で前処理し、まとめてリストとして返す関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_file_1(input_files):\n",
    "    res = []\n",
    "    for input_file in input_files:\n",
    "        with open(input_file, 'r', encoding='utf-8') as input_file_hd:\n",
    "            res += preprocess_text_1( input_file_hd.read() )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webスクレイピングで取得したファイルを前処理して、１行１単語の前処理済みテキストとして書き出す。<br>\n",
    "- mlit_y27.txt , mlit_y28.txt , mlit_y29.txt の３つを前処理 → mlit.txt へ\n",
    "- env_y27.txt , env_y28.txt , env_y29.txt の３つを前処理 → env.txt へ\n",
    "ここで作った mlit.txt と env.txt は学習の為のデータとして使います。<br>\n",
    "glob.globを使うと、存在するファイルのリストを作るのに便利。又、List型を文字列として連結する際はjoinを使うのが便利。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mlit.txt', 'w', encoding='utf-8') as mlit_out_file:\n",
    "    mlit_out_file.write( \"\\n\".join( preprocess_file_1(glob.glob('mlit_y2[789].txt')) ) + \"\\n\" )\n",
    "\n",
    "with open('env.txt', 'w', encoding='utf-8') as env_out_file:\n",
    "    env_out_file.write( \"\\n\".join( preprocess_file_1(glob.glob('env_y2[789].txt')) ) + \"\\n\" )\n",
    "\n",
    "[glob.glob('mlit.txt'),glob.glob('env.txt')]    # ファイルが出来たか確認用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証用データとして使う為、mlit,envの両方で、30年度のテキストの中ランダムな場所から、全体の1%を抜き出しておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.01\n",
    "\n",
    "with open('mlit_check.txt', 'w', encoding='utf-8') as mlit_out_file:\n",
    "    a = preprocess_file_1(['mlit_y30.txt'])\n",
    "    r = random.random()\n",
    "    mlit_out_file.write( (\"\\n\".join(a[int(r * (1.0 - test_ratio)):][0:int(test_ratio * len(a))])) + \"\\n\" )\n",
    "\n",
    "with open('env_check.txt', 'w', encoding='utf-8') as env_out_file:\n",
    "    a = preprocess_file_1(['env_y30.txt'])\n",
    "    r = random.random()\n",
    "    env_out_file.write( (\"\\n\".join(a[int(r * (1.0 - test_ratio)):][0:int(test_ratio * len(a))])) + \"\\n\" )\n",
    "\n",
    "[glob.glob('mlit_check.txt'),glob.glob('env_check.txt')]    # ファイルが出来たか確認用"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
